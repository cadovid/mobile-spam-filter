{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a mobile spam filter with the multinomial Naive Bayes algorithm\n",
    "\n",
    "To classify SMS messages as `spam` or `non-spam`, a computer:\n",
    "\n",
    "- Learns how humans classify messages.\n",
    "- Uses that human knowledge to estimate probabilities for new messages — probabilities for spam and non-spam.\n",
    "- Classifies a new message based on these probability values: if the probability for `spam` is greater, then it classifies the message as `spam`. Otherwise, it classifies it as `non-spam` (if the two probability values are equal, then we may need a human to classify the message).\n",
    "\n",
    "With this into account, our task is to \"teach\" the computer how to classify messages. To do that, we'll use the multinomial Naive Bayes algorithm along with a dataset of 5,572 SMS messages that are already classified by humans.\n",
    "\n",
    "The dataset can be downloaded from [The UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/sms+spam+collection). The data collection process is described in more details on [this page](http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/#composition), where it can also be found some of the authors' papers.\n",
    "\n",
    "### Exploring the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nr rows: 5572.\tNr columns: 2\n",
      "Nr of spam SMS: 747.\tNr of non-spam SMS: 4825\n",
      "Percentage of spam SMS: 13.41.\tPercentage of non-spam SMS: 86.59\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sms_df = pd.read_csv('SMSSpamCollection', sep='\\t', header=None, names=['Label', 'SMS'])\n",
    "rows, columns = sms_df.shape\n",
    "print('Nr rows: {}.\\tNr columns: {}'.format(rows, columns))\n",
    "print('Nr of spam SMS: {}.\\tNr of non-spam SMS: {}'.format((sms_df['Label'] == 'spam').sum(), (sms_df['Label'] == 'ham').sum()))\n",
    "print('Percentage of spam SMS: {}.\\tPercentage of non-spam SMS: {}'.format(round(((sms_df['Label'] == 'spam').sum() / len(sms_df)) * 100, 2), round(((sms_df['Label'] == 'ham').sum() / len(sms_df)) * 100, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the spam filter: creating train and test sets\n",
    "\n",
    "When creating software, a good rule of thumb is that designing the test comes before creating the software. Once our spam filter is done, we'll need to test how good it is with classifying _new messages_. To test the spam filter, we're first going to split our dataset into two categories:\n",
    "\n",
    "- A __training set__, which we'll use to \"train\" the computer how to classify messages.\n",
    "- A __test set__, which we'll use to test how good the spam filter is with classifying new messages.\n",
    "\n",
    "We're going to keep 80% of our dataset for training, and 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_sms shape: (4458, 2).\n",
      "test_sms shape: (1114, 2)\n"
     ]
    }
   ],
   "source": [
    "# Training datatest\n",
    "train_sms = sms_df.sample(frac=0.8, random_state=1)\n",
    "\n",
    "# Testing datatest\n",
    "test_sms = sms_df.sample(frac=0.2, random_state=1)\n",
    "\n",
    "# Reseting index\n",
    "train_sms.reset_index(drop=True, inplace=True)\n",
    "test_sms.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Checking outputs\n",
    "print('train_sms shape: {}.\\ntest_sms shape: {}'.format(train_sms.shape, test_sms.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_sms percentages:\n",
      "ham     86.54105\n",
      "spam    13.45895\n",
      "Name: Label, dtype: float64\n",
      "test_sms percentages:\n",
      "ham     86.804309\n",
      "spam    13.195691\n",
      "Name: Label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Exploring train and test datasets\n",
    "print('train_sms percentages:')\n",
    "print((train_sms['Label'].value_counts(normalize=True)) * 100)\n",
    "print('test_sms percentages:')\n",
    "print((test_sms['Label'].value_counts(normalize=True)) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The percentages obtained for both datasets are similar to the percentages obtained for the full dataset. Thus, we can assure the good aproximation to the data. We kept `random_state=1` for reproducibility reasons.\n",
    "\n",
    "### Building the spam filter: cleaning the dataset\n",
    "\n",
    "The next big step is to use the training set to teach the algorithm to classify new messages. When a new message comes in, our Naive Bayes algorithm will make the classification based on the results it gets to the probability equations following the Bayes' Theorem.\n",
    "\n",
    "To calculate all these probabilities, we'll first need to perform a bit of data cleaning to bring the data in a format that will allow us to extract easily all the information we need.\n",
    "\n",
    "The `SMS` column will be replaced by a series of new columns, where each column represents a unique word from the vocabulary. Each row describes a single message. All words in the vocabulary are in lower case and punctuation is not taken into account anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                         Yep, by the pretty sculpture\n",
      "1        Yes, princess. Are you going to make me moan?\n",
      "2                           Welp apparently he retired\n",
      "3                                              Havent.\n",
      "4    I forgot 2 ask ü all smth.. There's a card on ...\n",
      "Name: SMS, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Before cleaning\n",
    "print(train_sms['SMS'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                         yep  by the pretty sculpture\n",
      "1        yes  princess  are you going to make me moan \n",
      "2                           welp apparently he retired\n",
      "3                                              havent \n",
      "4    i forgot 2 ask ü all smth   there s a card on ...\n",
      "Name: SMS, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def erase_punc(string_):\n",
    "    return re.sub('\\W', ' ', string_)\n",
    "\n",
    "train_sms['SMS'] = train_sms['SMS'].apply(erase_punc).str.lower()\n",
    "\n",
    "# After cleaning\n",
    "print(train_sms['SMS'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the train vocabulary: 7783\n"
     ]
    }
   ],
   "source": [
    "# Creating vocabulary\n",
    "train_sms['SMS'] = train_sms['SMS'].str.split()\n",
    "vocabulary = []\n",
    "for row in train_sms['SMS']:\n",
    "    for word in row:\n",
    "        vocabulary.append(word)\n",
    "vocabulary = set(vocabulary) # For removing duplicates\n",
    "vocabulary = list(vocabulary)\n",
    "print('Length of the train vocabulary: {}'.format(len(vocabulary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating dictionary with word frequencies\n",
    "len_train = len(train_sms['SMS'])\n",
    "word_counts_per_sms = {word: [0] * len_train for word in vocabulary}\n",
    "\n",
    "for index, row in enumerate(train_sms['SMS']):\n",
    "    for word in row:\n",
    "        word_counts_per_sms[word][index] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0  00  000  000pes  008704050406  0089  01223585334  02  0207  02072069400  \\\n",
      "0  0   0    0       0             0     0            0   0     0            0   \n",
      "1  0   0    0       0             0     0            0   0     0            0   \n",
      "2  0   0    0       0             0     0            0   0     0            0   \n",
      "3  0   0    0       0             0     0            0   0     0            0   \n",
      "4  0   0    0       0             0     0            0   0     0            0   \n",
      "\n",
      "  ...  zindgi  zoe  zogtorius  zouk  zyada  é  ú1  ü  〨ud  鈥  \n",
      "0 ...       0    0          0     0      0  0   0  0    0  0  \n",
      "1 ...       0    0          0     0      0  0   0  0    0  0  \n",
      "2 ...       0    0          0     0      0  0   0  0    0  0  \n",
      "3 ...       0    0          0     0      0  0   0  0    0  0  \n",
      "4 ...       0    0          0     0      0  0   0  2    0  0  \n",
      "\n",
      "[5 rows x 7783 columns]\n"
     ]
    }
   ],
   "source": [
    "# Creating DataFrame from dictionary\n",
    "word_counts = pd.DataFrame(word_counts_per_sms)\n",
    "print(word_counts.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Resulting training dataset\n",
    "train_sms_clean = pd.concat([train_sms, word_counts], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we're done with data cleaning and have a training set to work with, we can begin creating the spam filter.\n",
    "\n",
    "### Building the spam filter: calculating constants\n",
    "\n",
    "As a start, let's first calculate:\n",
    "\n",
    "- `P(Spam)`: probability a message is labelled as spam.\n",
    "- `P(Ham)`: probability a message is labelled as non-spam.\n",
    "- `NSpam`: the number of words in all the spam messages -not unique words-.\n",
    "- `NHam`: the number of words in all the non-spam messages -not unique words-.\n",
    "- `NVocabulary`: the number of unique words in all the messages.\n",
    "\n",
    "We'll also use Laplace smoothing as additive smoothing and set α=1.\n",
    "\n",
    "All of these values will be used to calculate the probabilities of new messages with same values, therefore they are called __constants__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_spam value: 0.13458950201884254\n",
      "p_ham value: 0.8654104979811574\n"
     ]
    }
   ],
   "source": [
    "p_spam = (train_sms_clean['Label'] == 'spam').sum() / len(train_sms_clean)\n",
    "p_ham = (train_sms_clean['Label'] == 'ham').sum() / len(train_sms_clean)\n",
    "print('p_spam value: {}\\np_ham value: {}'.format(p_spam, p_ham))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_spam: 15190\n",
      "n_ham: 57237\n",
      "n_vocabulary: 7783\n"
     ]
    }
   ],
   "source": [
    "spam_sms = train_sms_clean[train_sms_clean['Label'] == 'spam'].copy()\n",
    "ham_sms = train_sms_clean[train_sms_clean['Label'] == 'ham'].copy()\n",
    "n_spam = spam_sms['SMS'].apply(len).sum() # SMS column is splitted\n",
    "n_ham = ham_sms['SMS'].apply(len).sum() # SMS column is splitted\n",
    "n_vocabulary = len(vocabulary)\n",
    "print('n_spam: {}\\nn_ham: {}\\nn_vocabulary: {}'.format(n_spam, n_ham, n_vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the spam filter: calculating parameters\n",
    "\n",
    "We now can use our training set to calculate the probability for each word in our vocabulary. In more technical language, the probability values that `P(wi|Spam)` and `P(wi|Ham)` will take are called __parameters__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialization\n",
    "parameters_spam = {word: 0 for word in vocabulary}\n",
    "parameters_ham = {word: 0 for word in vocabulary}\n",
    "\n",
    "# Function definition\n",
    "def compute_parameters(dataset, vocabulary, alpha, n_dataset, n_vocabulary, params_dict):\n",
    "    for word in vocabulary:\n",
    "        nr_words = dataset[word].sum()\n",
    "        parameter = (nr_words + alpha) / (n_dataset + (alpha * n_vocabulary))\n",
    "        params_dict[word] = parameter\n",
    "\n",
    "# Compute\n",
    "compute_parameters(spam_sms, vocabulary, alpha, n_spam, n_vocabulary, parameters_spam)\n",
    "compute_parameters(ham_sms, vocabulary, alpha, n_ham, n_vocabulary, parameters_ham)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the spam filter\n",
    "\n",
    "Now that we've calculated all the constants and parameters we need, we can start creating the spam filter. The spam filter can be understood as a function that:\n",
    "\n",
    "- Takes in as input a new message (w1, w2, ..., wn)\n",
    "- Calculates P(Spam|w1, w2, ..., wn) and P(Ham|w1, w2, ..., wn)\n",
    "- Compares the values of P(Spam|w1, w2, ..., wn) and P(Ham|w1, w2, ..., wn), and:\n",
    "    - If P(Ham|w1, w2, ..., wn) > P(Spam|w1, w2, ..., wn), then the message is classified as ham.\n",
    "    - If P(Ham|w1, w2, ..., wn) < P(Spam|w1, w2, ..., wn), then the message is classified as spam.\n",
    "    - If P(Ham|w1, w2, ..., wn) = P(Spam|w1, w2, ..., wn), then the algorithm may request human help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(Spam|message): 1.3481290211300841e-25\n",
      "P(Ham|message): 1.9368049028589875e-27\n",
      "Label: Spam\n",
      "P(Spam|message): 2.4372375665888117e-25\n",
      "P(Ham|message): 3.687530435009238e-21\n",
      "Label: Ham\n"
     ]
    }
   ],
   "source": [
    "def classify(message):\n",
    "\n",
    "    message = re.sub('\\W', ' ', message)\n",
    "    message = message.lower()\n",
    "    message = message.split()\n",
    "\n",
    "    p_spam_given_message = p_spam\n",
    "    p_ham_given_message = p_ham\n",
    "    \n",
    "    for word in message:\n",
    "        if word in parameters_spam:\n",
    "            p_spam_given_message *= parameters_spam[word]\n",
    "        if word in parameters_ham:\n",
    "            p_ham_given_message *= parameters_ham[word]\n",
    "    \n",
    "    print('P(Spam|message): {}\\nP(Ham|message): {}'.format(p_spam_given_message, p_ham_given_message))\n",
    "\n",
    "    if p_ham_given_message > p_spam_given_message:\n",
    "        print('Label: Ham')\n",
    "    elif p_ham_given_message < p_spam_given_message:\n",
    "        print('Label: Spam')\n",
    "    else:\n",
    "        print('Equal proabilities, have a human classify this!')\n",
    "        \n",
    "\n",
    "# Clasification examples\n",
    "spam_message = \"WINNER!! This is the secret code to unlock the money: C3421.\"\n",
    "ham_message = \"Sounds good, Tom, then see u there\"\n",
    "classify(spam_message)\n",
    "classify(ham_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the spam filter\n",
    "\n",
    "We'll now try to determine how well the spam filter does on our test set of 1,114 messages. The algorithm will output a classification label for every message in our test set, which we'll be able to compare with the actual label (given by a human).\n",
    "\n",
    "We can compare the predicted values with the actual values to measure how good our spam filter is with classifying new messages. To make the measurement, we'll use __accuracy__ as a metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ham                           967\n",
      "spam                          145\n",
      "needs human classification      2\n",
      "Name: predicted, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def classify_test_set(message):\n",
    "\n",
    "    message = re.sub('\\W', ' ', message)\n",
    "    message = message.lower()\n",
    "    message = message.split()\n",
    "\n",
    "    p_spam_given_message = p_spam\n",
    "    p_ham_given_message = p_ham\n",
    "\n",
    "    for word in message:\n",
    "        if word in parameters_spam:\n",
    "            p_spam_given_message *= parameters_spam[word]\n",
    "        if word in parameters_ham:\n",
    "            p_ham_given_message *= parameters_ham[word]\n",
    "\n",
    "    if p_ham_given_message > p_spam_given_message:\n",
    "        return 'ham'\n",
    "    elif p_spam_given_message > p_ham_given_message:\n",
    "        return 'spam'\n",
    "    else:\n",
    "        return 'needs human classification'\n",
    "\n",
    "# Test algorithm\n",
    "test_sms['predicted'] = test_sms['SMS'].apply(classify_test_set)\n",
    "print(test_sms['predicted'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total test values: 1114\n",
      "Correct classified values: 1106\n",
      "Incorrect classified values: 8\n",
      "Test accuracy: 0.992818671454219\n"
     ]
    }
   ],
   "source": [
    "# Compute accuracy\n",
    "correct = 0\n",
    "total = len(test_sms)\n",
    "\n",
    "for row in test_sms.itertuples(index=False):\n",
    "    if row[0] == row[2]:\n",
    "        correct += 1\n",
    "\n",
    "accuracy = correct / total\n",
    "\n",
    "# Outputs\n",
    "print('Total test values: {}\\nCorrect classified values: {}\\nIncorrect classified values: {}'.format(total, correct, total - correct))\n",
    "print('Test accuracy: {}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is close to 99.28%, which is really good. Our spam filter looked at 1,114 messages that it hasn't seen in training, and classified 1,106 correctly and 8 incorrectly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
